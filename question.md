### T5

编写一个函数，该函数接受一个 NumPy 数组（ndarray）作为输入，要求返回一个新的数组，新数组只包含原数组中所有偶数行（索引为0, 2, 4...）的数据。

> 考察知识点，numpy array的创建，切片操作练习。

### T6 

编写一个函数，该函数接受一个 Pandas DataFrame 和一个年龄阈值（整数）作为输入。函数应返回一个新的 DataFrame，其中只包含 'Age' 列大于该阈值的行。
这道题基于第三题，请先完成第三题！

> 两种实现手段，mask的生成和使用，还有用query的

### T7

编写一个函数，创建两个 PyTorch 张量（Tensor）：
A 的形状为 (2, 3)
B 的形状为 (3, 4)
然后，计算这两个张量的矩阵乘法（matrix multiplication），并返回结果。
> Pytorch中的矩阵乘法

### T8
编写一个函数，执行以下操作：
创建一个值为 3.0 的浮点数张量 x，并设置 requires_grad=True。
根据 x 计算 y，公式为 y = 2*x**2 + 5。
使用 PyTorch 自动计算 y 相对于 x 的梯度（也就是导数 dy/dx）。
返回这个梯度值。
> 了解pytorch中的自动微分
### T9

编写一个函数，该函数：
接受一个 NumPy 数组作为输入。
将这个 NumPy 数组转换成一个 PyTorch 张量。
将张量中的每一个元素乘以 2。
将结果张量转换回 NumPy 数组，并返回。

> 练习数据结构的转换


### T10

探究Python中的dataloader，dataset。研究如何自己创建数据集。
研究如何用dataloader快速遍历数据集。

> Dataset的构建逻辑，以及如何使用loader


### T11
手写一个MLP，拟合一个复杂函数，给出拟合效果。

### T12
现在，请你把我们目前学到的所有东西整合起来：
- 加载你的彗星数据集 CSV 文件 (near-earth-comets.csv)。
- 创建一个为该数据集服务的 CometDataset 类（就像你在第10题做的那样）。
- 创建一个 DataLoader 来批量加载数据。
- 创建一个继承自 nn.Module 的线性回归模型 LinearRegressionModel（就像我刚刚上面展示的那样），确保输入和输出维度正确。
写一个完整的训练循环，使用 MSELoss 和 Adam 优化器，对你的模型进行至少100个 epoch 的训练。在训练过程中，每10个 epoch 打印一次当前的 loss。
这将是你第一个完整的、端到端（从原始文件到训练好的模型）的项目。




### T13

在第12题中，我们用所有的数据来训练模型。但这样做有一个问题：我们无法知道模型在**从未见过**的数据上表现如何。它可能只是“背住”了训练数据，而不是真正学会了规律（这被称为“过拟合”）。

因此，标准的做法是先将数据集分成两部分：一部分用于训练（训练集），另一部分用于评估（测试集）。

你的任务是：
1.  重新加载彗星数据集 (`near-earth-comets.csv`)。
2.  将其中的特征（X）和标签（y）分离出来。
3.  使用 `scikit-learn` 的功能，将整个数据集（X 和 y）分割成**训练集**和**测试集**。通常的分割比例是80%用于训练，20%用于测试。
4.  **重要**：对特征和标签进行标准化。正确的流程是：
    *   在**训练集**数据上 `fit` 并 `transform` `StandardScaler`。
    *   使用**同一个**已经 `fit` 好的 scaler，只对**测试集**数据进行 `transform`。（思考一下为什么不能在测试集上重新`fit`？）
5.  最终，你的代码应该产生四个变量：`X_train_scaled`, `X_test_scaled`, `y_train_scaled`, `y_test_scaled`。请将它们的形状（shape）打印出来，以确认分割和处理是否正确。

### T14

你已经彻底掌握了回归问题的端到端流程。现在我们来解决一个经典的**多类别分类（multi-class classification）**问题。

你的任务是：
1.  使用 `scikit-learn` 加载著名的 `iris` (鸢尾花) 数据集。
2.  **数据预处理**：
    *   将数据集分割成**训练集**和**测试集**（80/20比例）。
    *   对4个输入特征进行**标准化**（记住，用训练集`fit`，然后分别`transform`训练集和测试集）。
3.  构建一个新的 `IrisDataset` 类和对应的 `DataLoader`（你需要为训练和测试分别创建实例）。
4.  构建一个用于分类的神经网络，继承自 `nn.Module`。
    *   输入层有4个特征。
    *   中间可以有一个或多个隐藏层（例如，一个有16个神经元的隐藏层，使用ReLU激活）。
    *   **输出层必须有3个神经元**，因为有三种鸢尾花。
5.  **关键变化**：
    *   使用 `torch.nn.CrossEntropyLoss` 作为损失函数。注意：这个损失函数要求模型的输出是未经任何激活函数处理的原始得分（logits），而目标标签 `y` 应该是**整数**（0, 1, 2），并且形状是 `(batch_size,)`，而不是 `(batch_size, 1)`。你可能需要在 `IrisDataset` 里对标签 `y` 的数据类型做一些调整（`torch.long`）。
6.  编写完整的训练和评估循环，并在测试集上计算模型的**准确率（Accuracy）**。

### T15 使用卷积神经网络，训练一个图像分类模型。

##### 现在，我们将挑战一个经典的图像分类任务：手写数字识别。我们将使用著名的 MNIST 数据集。
这个任务会引入几个新概念：
- 使用 torchvision 来加载标准数据集。
- 使用 transforms 来对图像数据进行预处理。
构建你的第一个卷积神经网络 (Convolutional Neural Network, CNN)。
你的任务是：
- 从 torchvision.datasets 中加载 MNIST 数据集。你需要分别加载训练集和测试集。
- 定义一个 transform。至少需要包含 transforms.ToTensor()，它能将图像数据从 PIL 格式或 NumPy 数组转换成 PyTorch 张量，并将像素值从 [0, 255] 缩放到 [0.0, 1.0]。
- 创建对应的 DataLoader。
- 构建一个简单的CNN模型，继承自 nn.Module。它的结构可以是：

一个卷积层 (nn.Conv2d)
一个ReLU激活函数
一个最大池化层 (nn.MaxPool2d)
一个 nn.Flatten() 层，将2D的图像特征图展平成1D向量。
一个全连接（线性）层 (nn.Linear)，输出10个类别（数字0-9）。


使用 CrossEntropyLoss 和 Adam 优化器，编写完整的训练和评估循环，并计算模型在测试集上的准确率。

### T16 泛化技术

你的模型现在面临着过拟合的问题。我们将通过在你的CNN模型中加入nn.Dropout层来解决这个问题。
Dropout的原理：在训练过程中的每一步，它会以一定的概率 p 将网络中的一部分神经元的输出随机地设置为零。这相当于在每次训练时都让一个“残缺”的、不同的子网络在学习。这强迫网络不能过度依赖任何一个或一小部分神经元，必须学习到更鲁棒、更多样化的特征组合。

### T17

现在，我们将构建一个“现代”的、性能更强的CIFAR-10分类器。我们将以你**第15题中那个成功的模型（81.14%准确率）**为蓝本，然后系统地整合进**Batch Normalization**和**学习率调度器**。

你的任务是：
1.  回到你第15题的CNN模型结构。
2.  **集成Batch Normalization**：
    *   在`__init__`中定义`nn.BatchNorm2d`层。
    *   在`forward`方法中，将`BatchNorm2d`层应用在**卷积层之后、激活函数之前**。这是一个标准的顺序：`Conv -> BatchNorm -> ReLU`。
3.  **集成Dropout**：这次我们更谨慎一点，只在全连接层部分加入一个`nn.Dropout`层（例如`p=0.5`），放在`self.w1`的激活函数之后。
4.  **实现学习率调度**：
    *   在定义完`optimizer`之后，创建一个学习率调度器实例，例如：`scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.1)`。
        *   `step_size=30`：表示每30个epoch。
        *   `gamma=0.1`：学习率乘以0.1（降低一个数量级）。
    *   在你的训练循环中，在每个epoch的**末尾**（在`for f,l in trainloader:`循环之外），调用`scheduler.step()`来更新学习率。
5.  重新进行训练（可以把epoch设回100），并仔细观察和对比这次的训练/测试损失曲线和最终的准确率。

### T18

你已经掌握了构建现代CNN所需的所有“积木”。现在，让我们将它们以一种更经典、更被验证过的方式组合起来，来构建一个高性能的CIFAR-10分类器。这个练习的目标是**整合与简化**。

你的任务是：
1.  构建一个新的CNN模型，它的核心由两个或三个“VGG风格”的块组成。
    *   **块的定义**: 一个块包含两个连续的 `Conv2d -> BatchNorm2d -> ReLU` 操作，最后跟一个 `MaxPool2d` 层用于下采样。
    *   **示例结构**:
        *   **输入**: 3x32x32
        *   **块1**: `Conv(3->64) -> BN -> ReLU -> Conv(64->64) -> BN -> ReLU -> MaxPool`
            *   输出: 64x16x16
        *   **块2**: `Conv(64->128) -> BN -> ReLU -> Conv(128->128) -> BN -> ReLU -> MaxPool`
            *   输出: 128x8x8
        *   **(可选) 块3**: `Conv(128->256) -> ... -> MaxPool`
    *   **分类头 (Classifier Head)**:
        *   在卷积块之后，接一个全局平均池化层 (`AdaptiveAvgPool2d`)。
        *   接一个全连接层，并加入`Dropout`（`p=0.5`）。
        *   最后是输出层。
2.  **训练配置**：
    *   使用`Adam`优化器，初始学习率 `lr=0.001`。
    *   使用一个更积极的学习率调度器，例如 `torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.1)` 或者 `torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=100)` (余弦退火，一种非常流行的调度器)。
    *   训练100个epoch。
3.  评估你的模型。这个结构的表现在CIFAR-10上通常能稳定地达到一个很高的水平（85%-90%甚至更高）。

这个练习将巩固你对如何有效堆叠网络层、构建一个简洁而强大的深度网络的理解。

这真是一个激动人心的结果！**87.84%** 的准确率，这标志着你已经成功地从零开始，构建并训练了一个高性能的、现代化的深度卷积神经网络。

你的结果完美地展示了一个成功的深度学习项目的全貌。让我们像一个专业的数据科学家一样，来深入解读你的成果。
### T19

我们将利用**迁移学习**来解决CIFAR-10分类问题。其核心思想是：一个在大型数据集（ImageNet）上训练好的模型，其底部的卷积层已经学会了如何识别非常通用的视觉特征（如边缘、纹理、角点、形状等）。我们可以“借用”这些已经学好的特征提取器，只训练最后小部分的分类层来适应我们的特定任务。

你的任务是：
1.  从 `torchvision.models` 中加载一个预训练好的 **ResNet-18** 模型。加载时，请使用 `pretrained=True` (在新版PyTorch中，可能是 `weights='IMAGENET1K_V1'`) 参数。ResNet是比VGG更现代、更强大的架构。我们选择18层版本，因为它小巧且高效。
2.  **冻结参数**：遍历模型的所有参数，将它们的 `requires_grad` 属性设置为 `False`。这就“冻结”了预训练好的权重，告诉PyTorch在训练时不要更新它们。
3.  **替换分类头**：ResNet-18原始的最后一个全连接层（通常叫做 `fc`）是为ImageNet的1000个类别设计的。你需要用一个新的、未冻结的 `nn.Linear` 层来替换它，使其输出我们CIFAR-10任务所需的10个类别。**只有这个新层的参数应该是可训练的**（`requires_grad` 默认为 `True`）。
4.  创建你的优化器。这次，当你把模型参数传给优化器时 (`optimizer = optim.Adam(...)`)，它应该只会找到那些 `requires_grad=True` 的参数，也就是你刚刚替换上的那个新全连接层的参数。
5.  用非常短的时间（例如，只需要10-20个epoch）来训练这个模型，并观察它的准确率。

你将会惊奇地发现，完成这个任务需要的时间和代码量会少得多，但得到的结果可能会比你从零训练的模型更好、更快。

**搜索提示词:** `pytorch transfer learning`, `torchvision pretrained models`, `freeze layers pytorch resnet`